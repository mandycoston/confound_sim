---
title: "High Dimensional Synthetic Experiments"
output: html_notebook
---

This notebook runs a simulation based on Edward's setup.

```{r}
library(tidyverse)
library(glmnet)
library(doParallel)
source("utils.R")
```

```{r}
results_folder <- "results/highdim/param1/"
start_time <- Sys.time()
#set.seed(3)
set.seed(100)
results <- tibble()
n <- 4 * 1000
n_sim <- 140
d <- 500
dim_z <- 10 # dimension of hidden confounder z
p <- d - dim_z # dimension of v
zeta <- dim_z # number of non-zero predictors in z
gamma <- 25 # number of non-zero predictors in v
beta <- gamma + zeta
# beta <- min(round(sqrt(gamma*n/log(d))), d)
# beta <- min(round(sqrt(gamma*n*log(p))/log(d)), d)
# alpha <- round(min(1/4*n*log(p)/(log(d)^2), 1/4*gamma*n*log(p)/(log(d)^2*(d-p)), d)) #1/4 becuase 4*1000 vs 1000?
alpha <- 25
s <- sort(rep(1:4, n / 4))

# parallelize
registerDoParallel(cores = 15)

results <- foreach (sim_num = 1:n_sim) %dopar% {
  x <- matrix(rnorm(n * d), n, d)
  v <- x[, (dim_z + 1):d]
  mu0 <- as.numeric(x %*% rep(c(1, 0), c(beta, d - beta)))
  nu <- as.numeric(x %*% rep(c(0, 1, 0), c(dim_z, beta - dim_z, d - beta)))
  prop <- sigmoid(as.numeric(x %*% rep(c(1, 0), c(alpha, d - alpha))) / sqrt(4 * alpha))
  a <- rbinom(n, 1, prop)
  y0 <- mu0 + rnorm(n, sd = sqrt(sum(mu0^2) / (n * 2)))

  # propvhat <- as.numeric(predict(cv.glmnet(v, a, subset = (s = 1), famliy = "binomial"), newx = v, type = "response", s = "lambda.min"))

  # stage 1
  prop_lasso <- cv.glmnet(x[s == 1, ], a[s == 1], family = "binomial")
  prophat <- as.numeric(predict(prop_lasso, newx = x, type = "response"))

  mu_lasso <- cv.glmnet(x[((s == 2) & (a == 0)), ], y0[((s == 2) & (a == 0))])
  muhat <- as.numeric(predict(mu_lasso, newx = x))

  bchat <- (1 - a) * (y0 - muhat) / (1 - prophat) + muhat
  bc_true <- (1 - a) * (y0 - mu0) / (1 - prop) + mu0

  # stage 2
  conf_lasso <- cv.glmnet(v[((s == 3) & (a == 0)), ], y0[((s == 3) & (a == 0))])
  conf <- predict(conf_lasso, newx = v, s = "lambda.min")
  conf1se <- predict(conf_lasso, newx = v)

  pl_lasso <- cv.glmnet(v[s == 3, ], muhat[s == 3])
  pl <- predict(pl_lasso, newx = v, s = "lambda.min")
  pl1se <- predict(pl_lasso, newx = v)

  bc_lasso <- cv.glmnet(v[s == 3, ], bchat[s == 3])
  bc <- predict(bc_lasso, newx = v, s = "lambda.min")

  bct_lasso <- cv.glmnet(v[s == 3, ], bc_true[s == 3])
  bct <- predict(bct_lasso, newx = v, s = "lambda.min")

  # replicating edward's
  tau <- as.numeric(x %*% rep(c(1, 0), c(gamma, d - gamma)))
  mu1 <- mu0 + tau
  y <- (1 - a) * mu0 + a * mu1 + rnorm(n, sd = sqrt(sum(mu0^2) / (n * 2)))

  mu0hat <- muhat
  # mu1hat <- predict(cv.glmnet(x, y, subset = ((s == 2) & (a == 1))), newx = x, s = "lambda.min")
  mu1hat <- predict(cv.glmnet(x[((s == 2) & (a == 1)), ], y[((s == 2) & (a == 1))]), newx = x, s = "lambda.min")
  plugin <- mu1hat - mu0hat
  pseudo <- ((a - prophat) / (prophat * (1 - prophat))) * (y - a * mu1hat - (1 - a) * mu0hat) + mu1hat - mu0hat
  # drl_lasso <- cv.glmnet(x, pseudo, subset = (s == 3))
  drl_lasso <- cv.glmnet(x[s == 3, ], pseudo[s == 3])
  drl <- predict(drl_lasso, newx = x, s = "lambda.min")

  tibble(
    "mse" = c(
      mean((conf - nu)[s == 4]^2),
      mean((pl - nu)[s == 4]^2),
      mean((bc - nu)[s == 4]^2),
      mean((bct - nu)[s == 4]^2),
      mean((conf1se - nu)[s == 4]^2),
      mean((pl1se - nu)[s == 4]^2)
    ),
    "method" = c("conf", "pl", "bc", "bct", "conf1se", "pl1se"),
    "sim" = sim_num
  )
}
  saveRDS(tibble(    
    "dim" = d,
    "n_in_each_fold" = n/4,
    "dim_z" = dim_z,
    "p" = p,
    "zeta" = zeta,
    "gamma" = gamma,
    "beta" = beta,
    "alpha" = alpha), glue::glue(results_folder, "parameters.Rds"))
  
saveRDS(bind_rows(results), glue::glue(results_folder, "results2.Rds"))

task_time <- difftime(Sys.time(), start_time)
print(task_time)
```

