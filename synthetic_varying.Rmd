---
title: "Low-Dimensional Synthetic Experiments for Predictions with Runtime Confounding"
output: html_notebook
---

```{r}
library(tidyverse)
library(np)
library(glue)
source("utils.R")
```

```{r}
compute_mu <- function(v, z) {
  return(.005 * v + .5 * z + .2)
}

# inputs: predictor v
# returns a number in [0,1]
compute_nu <- function(v, c) {
  p1 <- compute_mu(v, 1) * (c * sigmoid(v) + (1 - c) * .5)
  p0 <- compute_mu(v, 0) * (1 - (c * sigmoid(v) + (1 - c) * .5))
  return(p1 + p0)
}
```

```{r}
set.seed(99)

# number of simulations
n_sim <- 2 #500

# number of training points in each simulation
n <- 5000

# parameter controlling dependence of z1 on v1
c <- .4

# standard deviation in the predictor v1
sd_v1 <- 10

# weights in the linear propensity model 
a1 <- c(-2, .1, 4)
# a2 <- c(-2, .05, 5)
a <- a1

# discard all training points with propensity higher
# than prop_cutoff
prop_cutoff <- 0.95

# create a test set to be used for all simulations
v <- seq(-30, 30, .01)
test <- tibble(
  v1 = v,
  nu = compute_nu(v, c)
)

n_test <- nrow(test)
# simulate
for (sim in c(1:n_sim)) {
  v1 <- rnorm(n = n, mean = 0, sd = sd_v1)
  z1 <- rbinom(n = n, size = 1, prob = c * sigmoid(v1) + (1 - c) * .5)
  prop <- sigmoid(as.numeric(as.matrix(cbind(1, v1, z1)) %*% a))
  A <- rbinom(n = n, size = 1, prob = prop)

  tibble(
    v1 = v1,
    z1 = z1,
    prop = prop,
    A = A
  ) -> df

  # add values of true regression models to dataframe
  df %>%
    dplyr::mutate(
      mu = purrr::pmap_dbl(list(v1, z1), compute_mu),
      nu = purrr::pmap_dbl(list(v1, c), compute_nu)
    ) -> df

 # mean((df$mu - df$nu)^2) # is this too small of a difference?

  df %>%
    dplyr::mutate(
      partition = "train", 
      y0 = rbinom(n = n, size = 1, prob = mu),
      y0cat = if_else(y0 == 1, "one", "zero")
    ) -> df

  df %>%
    dplyr::filter(prop < prop_cutoff) -> df

  # create dataframe to store results
  pred <- tibble(
    "v1" = numeric(),
    "nu" = numeric(),
    "sim_num" = numeric(),
    "pred" = numeric(),
    "eps_n_exp" = numeric(),
    "eps_sd" = numeric()
  )
  
  # mse <- tibble(
  #   "method" = character(),
  #   "mse" = numeric(),
  #   "low" = numeric(),
  #   "high" = numeric(),
  #   "eps_n_exp" = numeric(),
  #   "eps_sd" = numeric(),
  #   "c" = numeric(),
  #   "a" = character(),
  #   "prop_cutoff" = numeric()
  # )

  # Stage 1
  # simulate noise for nuisance functions
  # eps_n_exp <- 0.25
  # eps_sd <- 1
  # for (eps_n_exp in c(.5, .4, .3, .25, .2, .15, .1)) {
  #   for (eps_sd in c(1, 2, 5)) {
  for (eps_n_exp in c(.5, .4)) {
    for (eps_sd in c(1, 2)) {
      # create pseudo outcomes (in lieu of first stage)

      eps_mu <- rnorm(nrow(df), mean = 0, sd = eps_sd) / (nrow(df)^(eps_n_exp))
      eps_pi <- rnorm(nrow(df), mean = 0, sd = eps_sd) / (nrow(df)^(eps_n_exp))

      df %>%
        dplyr::mutate(
          mueps = sigmoid(logit(mu) + eps_mu),
          propeps = sigmoid(logit(prop) + eps_pi),
          bceps = mueps + (1 - A) / (1 - propeps) * (y0 - mueps)
        ) -> df

      # Stage 2: regress on V
      # regress plugin
      bw_pl <- np::npregbw(formula = mueps ~ v1, data = select(
        filter(df, partition == "train"),
        mueps, v1
      ))
      
      reg_pl <- np::npreg(
        bws = bw_pl,
        data = select(
          filter(df, partition == "train"),
          mueps, v1
        ))

      # regress bias-corrected
      bw_bc <- np::npregbw(formula = bceps ~ v1, data = select(
        filter(
          df,
          partition == "train"
        ),
        bceps, v1
      ))

      reg_bc <- np::npreg(
        bws = bw_bc,
        data = select(
          filter(df, partition == "train"),
          bceps, v1
        )
      )

      # regress confounded
      bw_conf <- np::npregbw(formula = y0 ~ v1, data = select(
        filter(
          df,
          partition == "train",
          A == 0
        ),
        y0, v1
      ))

      reg_conf <- np::npreg(
        bws = bw_conf,
        data = select(
          filter(
            df,
            partition == "train",
            A == 0
          ),
          y0, v1
        )
      )

      # df %>%
      #   dplyr::mutate(
      #     pl = predict(reg_pl, newdata = df),
      #     bc = predict(reg_bc, newdata = df),
      #     conf = predict(reg_conf, newdata = df),
      #     const = mean(filter(df, partition == "train", A == 0)$y0)
      #   ) -> df
      
      test %>%
        dplyr::mutate(
          pl = predict(reg_pl, newdata = test),
          bc = predict(reg_bc, newdata = test),
          conf = predict(reg_conf, newdata = test),
          const = mean(filter(df, partition == "train", A == 0)$y0)
        ) -> test

    pred <- rbind(pred, tibble(
  "v1" = dplyr::pull(test, v1),
  "nu" = dplyr::pull(test, nu),
  "sim_num" = sim,
  "pl" = dplyr::pull(test, pl),
  "bc" = dplyr::pull(test, bc),
  "conf" = dplyr::pull(test, conf),
  "eps_n_exp" = eps_n_exp,
  "eps_sd" = eps_sd
))
      # mse_pl <- compute_mse(preds = test$pl, label = test$nu)
      # mse_bc <- compute_mse(preds = test$bc, label = test$nu)
      # mse_conf <- compute_mse(preds = test$conf, label = test$nu)
      # mse_const <- compute_mse(preds = test$const, label = test$nu)
      # mse <- rbind(
      #   mse,
      #   tibble(
      #     "method" = c("constant", "confounded", "plug-in", "bias-corrected"),
      #     "mse" = c(mse_const$mse, mse_conf$mse, mse_pl$mse, mse_bc$mse),
      #     "low" = c(mse_const$low, mse_conf$low, mse_pl$low, mse_bc$low),
      #     "high" = c(mse_const$high, mse_conf$high, mse_pl$high, mse_bc$high),
      #     "eps_n_exp" = eps_n_exp,
      #     "eps_sd" = eps_sd,
      #     "c" = c,
      #     "a" = toString(a),
      #     "prop_cutoff" = prop_cutoff
      #   )
      # )
    }
  }
  # save the predictions to files
  saveRDS(pred, glue::glue("results/prop{prop_cutoff*100}/sim{sim}.Rds"))
}
```

```{r}
prop_cutoff <- .95
res <- readRDS(glue::glue("results/prop{prop_cutoff*100}/sim{sim}.Rds"))

for (i in c(1:(sim - 1))) {
  res <- rbind(res, readRDS(glue::glue("results/prop{prop_cutoff*100}/sim{i}.Rds")))
}

res %>%
  dplyr::group_by(eps_n_exp, eps_sd, v1) %>%
  dplyr::summarise(var_pl = var(pl),
                   var_bc = var(bc),
                   var_conf = var(conf),
                   # mean_pl = mean(pl),
                   # mean_bc = mean(bc),
                   # mean_conf = mean(conf),
                   # nu = mean(nu),
                   bias_pl = mean(pl) - mean(nu),
                   bias_bc = mean(bc) - mean(nu),
                   bias_conf = mean(conf) - mean(nu),
                   ) -> results

results %>% 
  dplyr::mutate(sqerr_pl = bias_pl^2 + var_pl, 
                sqerr_bc = bias_bc^2 + var_bc, 
                sqerr_conf = bias_conf^2 + var_conf, 
    w = dnorm(v1, mean = 0, sd = sd_v1)) -> results

results %>% 
  dplyr::ungroup() %>%
  dplyr::group_by(eps_n_exp, eps_sd) %>%
  dplyr::summarise(mse_pl = mean(w*sqerr_pl),
                mse_bc = mean(w*sqerr_bc),
                mse_conf = mean(w*sqerr_conf),
                vse_pl = var(w*sqerr_pl),
                vse_bc = var(w*sqerr_bc),
                vse_conf = var(w*sqerr_conf)) %>%
  dplyr::mutate(low_pl = mse_pl - 1.96 *sqrt(vse_pl/n_test),
                high_pl = mse_pl + 1.96 *sqrt(vse_pl/n_test),
                low_bc = mse_bc - 1.96 *sqrt(vse_bc/n_test),
                high_bc = mse_bc + 1.96 *sqrt(vse_bc/n_test),
                low_conf = mse_conf - 1.96 *sqrt(vse_conf/n_test),
                high_conf = mse_conf + 1.96 *sqrt(vse_conf/n_test))-> res_sum
```

```{r}
res_sum %>%
    ggplot(aes(x=eps_n_exp, y= mse, color = method, fill = method))  + geom_ribbon(aes(ymin=low, ymax=high), alpha = 0.4)+ geom_line()+ ylab('Mean Squared Error') + facet_grid(eps_sd ~.) + theme_grey(base_size=12)    + theme(legend.position = "bottom") 

```


