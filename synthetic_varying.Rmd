---
title: "Low-Dimensional Synthetic Experiments for Predictions with Runtime Confounding"
output: html_notebook
---

This file runs simulations for a setting with one hidden confounder and one predictor. 
The data-generating processing is specified in "regression_functions.R"
```{r}
library(tidyverse)
library(np)
library(glue)
library(doParallel)
source("utils.R")
source("regression_functions.R")
```

The below code chunk runs simulations and saves the predictions in {r results_folder}
```{r}
results_folder <- "results/mu_sigmoid/propa3/train500_ext/"
```


```{r}
start_time <- Sys.time()
set.seed(990)

# number of simulations
n_sim <- 500

# number of training points in each simulation
n <- 500

# parameter controlling dependence of z1 on v1
c <- .4

# standard deviation in the predictor v1
sd_v1 <- 10

# discard all training points with propensity higher
# than prop_cutoff
prop_cutoff <- 1

# create a test set to be used for all simulations
v <- seq(-30, 30, .01)
test <- tibble(
  v1 = v,
  nu = compute_nu(v, c)
)

n_test <- nrow(test)

# parallelize
registerDoParallel(cores = 15)

# simulate
foreach(sim = 1:n_sim) %dopar% {
  # create df = training data
  tibble(
    v1 = rnorm(n = n, mean = 0, sd = sd_v1),
    z1 = rbinom(n = n, size = 1, prob = c * sigmoid(v1) + (1 - c) * .5),
    prop = sigmoid(as.numeric(as.matrix(cbind(1, v1, z1)) %*% a)),
    A = rbinom(n = n, size = 1, prob = prop)
  ) -> df

  # add values of true regression models
  df %>%
    dplyr::mutate(
      mu = purrr::pmap_dbl(list(v1, z1), compute_mu),
      nu = purrr::pmap_dbl(list(v1, c), compute_nu)
    ) -> df

  # mean((df$mu - df$nu)^2) # is this too small of a difference?

  # sample outcome and filter out rows that will always get treated (if any)
  df %>%
    dplyr::mutate(
      partition = "train",
      y0 = rbinom(n = n, size = 1, prob = mu),
      y0cat = if_else(y0 == 1, "one", "zero")
    ) %>%
    dplyr::filter(prop < prop_cutoff) -> df

  # create dataframe to store prediction results
  pred <- tibble(
    "v1" = numeric(),
    "nu" = numeric(),
    "sim_num" = numeric(),
    "pred" = numeric(),
    "eps_n_exp" = numeric(),
    "eps_sd" = numeric(),
    "n_train" = numeric(),
    "vsd" = numeric()
  )

  # Stage 1: simulate noise for nuisance functions

  for (eps_n_exp in c(.5, .25, 0.2, 0.15, .1, 0.005)) {
    for (eps_sd_mu in c(5)) {
      for (eps_sd_pi in c(1, 5)) {
      print(glue::glue("eps_sd_mu is {eps_sd_mu}"))
      print(glue::glue("eps_n_exp is {eps_n_exp}"))

      # create pseudo outcomes (in lieu of first stage)
      eps_mu <- rnorm(nrow(df), mean = 0, sd = eps_sd_mu) / (nrow(df)^(eps_n_exp))
      eps_pi <- rnorm(nrow(df), mean = 0, sd = eps_sd_pi) / (nrow(df)^(eps_n_exp))

      df %>%
        dplyr::mutate(
          mueps = sigmoid(logit(mu) + eps_mu),
          propeps = pmin(sigmoid(logit(prop) + eps_pi), 0.999999),
          bceps = mueps + (1 - A) / (1 - propeps) * (y0 - mueps),
          bc_trueprop = mueps + (1 - A) / (1 - prop) * (y0 - mueps)
        ) -> df
      
      summary(df$mu - df$mueps)
      summary(df$prop - df$propeps)

      # Stage 2: regress on V
      # regress plugin
      bw_pl <- np::npregbw(formula = mueps ~ v1, data = select(
        filter(df, partition == "train"),
        mueps, v1
      ))

      reg_pl <- np::npreg(
        bws = bw_pl,
        data = select(
          filter(df, partition == "train"),
          mueps, v1
        )
      )

      # regress bias-corrected
      bw_bc <- np::npregbw(formula = bceps ~ v1, data = select(
        filter(
          df,
          partition == "train"
        ),
        bceps, v1
      ))

      reg_bc <- np::npreg(
        bws = bw_bc,
        data = select(
          filter(df, partition == "train"),
          bceps, v1
        )
      )

      # regress bias-corrected with true prop
      bw_bct <- np::npregbw(formula = bc_trueprop ~ v1, data = select(
        filter(
          df,
          partition == "train"
        ),
        bc_trueprop, v1
      ))

      reg_bct <- np::npreg(
        bws = bw_bct,
        data = select(
          filter(df, partition == "train"),
          bc_trueprop, v1
        )
      )

     # regress confounded
      bw_conf <- np::npregbw(formula = y0 ~ v1, data = select(
        filter(
          df,
          partition == "train",
          A == 0
        ),
        y0, v1
      ))

      reg_conf <- np::npreg(
        bws = bw_conf,
        data = select(
          filter(
            df,
            partition == "train",
            A == 0
          ),
          y0, v1
        )
      )

      test %>%
        dplyr::mutate(
          pl = predict(reg_pl, newdata = test),
          bc = predict(reg_bc, newdata = test),
          bct = predict(reg_bct, newdata = test),
          conf = predict(reg_conf, newdata = test),
          # const = mean(filter(df, partition == "train", A == 0)$y0)
        ) -> test

      pred <- rbind(pred, tibble(
        "v1" = dplyr::pull(test, v1),
        "nu" = dplyr::pull(test, nu),
        "sim_num" = sim,
        "pl" = dplyr::pull(test,     pl),
        "bc" = dplyr::pull(test, bc),
        "conf" = dplyr::pull(test, conf),
        "bct" = dplyr::pull(test, bct),
        "eps_n_exp" = eps_n_exp,
        "eps_sd_mu" = eps_sd_mu,
         "eps_sd_pi" = eps_sd_pi,
        "n_train" = n,
        "vsd" = sd_v1
      ))
    }
    }
  }

  pred %>%
    tidyr::pivot_longer(cols = pl:bct, names_to = "method", values_to = "pred") -> pred

  saveRDS(pred, glue::glue(results_folder, "sim{sim}.Rds"))
}
task_time <- difftime(Sys.time(), start_time)
print(task_time)
```
