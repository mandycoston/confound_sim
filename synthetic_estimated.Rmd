---
title: "Low-Dimensional Synthetic Experiments for Predictions with Runtime Confounding"
output: html_notebook
---



```{r}
library(tidyverse)
library(np)
library(glue)
source("utils.R")
source("regression_functions.R")
```


```{r}
set.seed(99)
n <- 15000
num <- 0

c <- .4

a <- c(1, .1, -2.75)
prop_cutoff <- 1
sd_v1 <- 10
v1 <- rnorm(n = n, mean = 0, sd = sd_v1)
z1 <- rbinom(n = n, size = 1, prob = c * sigmoid(v1) + (1 - c) * .5)
prop <- sigmoid(as.numeric(as.matrix(cbind(1, v1, z1)) %*% a))
A <- rbinom(n = n, size = 1, prob = prop)


tibble(
  v1 = v1,
  z1 = z1,
  prop = prop,
  A = A
) -> df

# regression models
df %>%
  dplyr::mutate(
    mu = purrr::pmap_dbl(list(v1, z1), compute_mu),
    nu = purrr::pmap_dbl(list(v1, c), compute_nu)
  ) -> df

train <- rbinom(n = n, size = 1, prob = .33)
nuis <- rbinom(n = n, size = 1, prob = .5)

df %>%
  dplyr::mutate(
    partition = if_else(train == 1, "train", if_else(nuis == 1, "nuis", "test")),
    y0 = rbinom(n = n, size = 1, prob = mu),
    y0cat = if_else(y0 == 1, "one", "zero")
  ) -> df

df %>%
  dplyr::filter(prop < prop_cutoff) -> df

# Stage 1: Estimate nuisance functions
# bw_mu <- np::npregbw(xdat = select(filter(df, partition == "nuis",
#                                           A == 0), v1, z1),
#                      ydat = pull(filter(df, partition == "nuis",
#                                           A == 0), y0))

bw_mu <- np::npregbw(
  formula = y0 ~ v1 + z1,
  data = filter(
    df, partition == "nuis",
    A == 0
  )
)

reg_mu <- np::npreg(
  bws = bw_mu,
  data = select(
    filter(
      df, partition == "nuis",
      A == 0
    ),
    y0, v1, z1
  ),
  newdata = select(
    df,
    y0, v1, z1
  ),
  y.eval = TRUE
)

# bw_pi <- np::npregbw(xdat = select(filter(df, partition == "nuis"), v1, z1),
#                      ydat = pull(filter(df, partition == "nuis"), A))

bw_pi <- np::npregbw(
  formula = A ~ v1 + z1,
  data = select(filter(df, partition == "nuis"), v1, z1, A)
)
reg_pi <- np::npreg(
  bws = bw_pi,
  data = select(
    filter(df, partition == "nuis"),
    A, v1, z1
  ),
  newdata = select(
    filter(df, partition != "nuis"),
    A, v1, z1
  ),
  y.eval = TRUE
)

df %>%
  dplyr::mutate(
    muest = predict(reg_mu, newdata = select(df, v1, z1)),
    propest = predict(reg_pi, newdata = df),
    bcest = muest + (1 - A) / (1 - propest) * (y0 - muest)
  ) -> df

# Stage 2: regress on V
# regress plugin
bw_pl <- np::npregbw(formula = muest ~ v1, data = select(
  filter(df, partition == "second_stage"),
  muest, v1
))

reg_pl <- np::npreg(
  bws = bw_pl,
  data = select(
    filter(df, partition == "second_stage"),
    muest, v1
  ),
  newdata = select(
    filter(df, partition == "test"),
    muest, v1
  ),
  y.eval = TRUE
)

# regress bias-corrected
bw_bc <- np::npregbw(formula = bcest ~ v1, data = select(
  filter(
    df,
    partition == "train"
  ),
  bcest, v1
))

reg_bc <- np::npreg(
  bws = bw_bc,
  data = select(
    filter(df, partition == "train"),
    bcest, v1
  ),
  newdata = select(
    filter(df, partition == "test"),
    bcest, v1
  ),
  y.eval = TRUE
)

# regress confounded
bw_conf <- np::npregbw(formula = y0 ~ v1, data = select(
  filter(
    df,
    partition == "train",
    A == 0
  ),
  y0, v1
))

reg_conf <- np::npreg(
  bws = bw_conf,
  data = select(
    filter(
      df,
      partition == "train",
      A == 0
    ),
    y0, v1
  ),
  newdata = select(
    filter(
      df,
      partition == "test",
      A == 0
    ),
    y0, v1
  ),
  y.eval = TRUE
)

df %>%
  dplyr::mutate(
    pl = predict(reg_pl, newdata = df),
    bc = predict(reg_bc, newdata = df),
    conf = predict(reg_conf, newdata = df),
    const = mean(filter(df, partition == "train", A == 0)$y0)
  ) -> df

df %>%
  dplyr::filter(partition == "test") -> test


```


The below code chunk runs simulations and saves the predictions in {r results_folder}
```{r}
results_folder <- "results/mu_sigmoid/train100/sim{sim}.Rds"
```

