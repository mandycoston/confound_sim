---
title: "Low Dim Linear Models"
output: html_notebook
---


This notebook is for debugging High Dimensional Synthetic Experiments. The bug was found! Glmnet does not support the "subset" flag so all code calls with that are not working as expected.
```{r}
library(tidyverse)
library(glmnet)
source("utils.R")
```

insights: for fixed dim z, as we increase dim v, the confounding model does better. This makes sense since z is explaining less of the variation in y0

As we vary z proportional to d, the confounding model consistently does worse. This is under a regime where propensity model solely uses z.
```{r}
set.seed(2)
results <- tibble()
n <- 4 * 1000
n_sim <- 100
for (sim_num in c(1:n_sim)) {
  #for (d in c(2, 5, 10, 20, 50)) {
  for (d in c(5)) {
   
    #d <- 50
    dim_z <- round(d/2) # dimension of hidden confounder z
    p <- d - dim_z # dimension of v
    zeta <- dim_z # number of non-zero predictors in z
    gamma <- p # number of non-zero predictors in v
    beta <- gamma + zeta
    alpha <- dim_z

    s <- sort(rep(1:4, n / 4))
    x <- matrix(rnorm(n * d), n, d)
    v <- x[, (dim_z + 1):d]
    xdf <- as.data.frame(x)
    z_names <- colnames(xdf)[1:dim_z]
    v_names <- colnames(xdf)[(dim_z + 1):d]
    vdf <- as.data.frame(v)
    mu0 <- as.numeric(x %*% rep(c(1, 0), c(beta, d - beta)))
    nu <- as.numeric(x %*% rep(c(0, 1, 0), c(dim_z, beta - dim_z, d - beta)))
    prop <- sigmoid(as.numeric(x %*% rep(c(1, 0), c(alpha, d - alpha))) / sqrt(4 * alpha))
    a <- rbinom(n, 1, prop)
    y0 <- mu0 + rnorm(n, sd = sqrt(sum(mu0^2) / (n * 2)))

    # propvhat <- as.numeric(predict(cv.glmnet(v, a, subset = (s = 1), famliy = "binomial"), newx = v, type = "response", s = "lambda.min"))

    # stage 1
    prop_model <- glm(a ~ ., data = xdf, subset = (s == 1), family = "binomial")
    prophat <- as.numeric(predict(prop_model, newdata = xdf, type = "response"))
    
    mu_model <- lm(y0 ~ ., data = xdf, subset = (s == 2 & a == 0))
    muhat <- as.numeric(predict(mu_model, newdata = xdf))
    
    prop_lasso <- cv.glmnet(x, a, subset = (s == 1), family = "binomial")
    prophat_lasso <- as.numeric(predict(prop_lasso, newx = x, type = "response"))
    
    mu_lasso <- cv.glmnet(x, y0, subset = ((s == 2) & (a == 0)))
    muhat_lasso <-as.numeric(predict(mu_lasso, newx = x))

    bchat <- (1 - a) * (y0 - muhat) / (1 - prophat) + muhat
    bchat_lasso <- (1 - a) * (y0 - muhat_lasso) / (1 - prophat_lasso) + muhat_lasso
    bc_true <- (1 - a) * (y0 - mu0) / (1 - prop) + mu0

    # stage 2
    conf_model <- lm(y0 ~ ., data = vdf, subset = ((s == 3) & (a == 0)))
    conf <- predict(conf_model, newdata = vdf)
    
    conf_lasso_model <- cv.glmnet(v, y0, subset = ((s == 3) & (a == 0))) ### BUGGY
    conf_lasso <- predict(conf_lasso_model, newx = v, s = "lambda.min")
    
    conf_lasso_model2 <- cv.glmnet(v[((s == 3) & (a == 0)),], y0[((s == 3) & (a == 0))]) ##### BUG SOLVED! This is the correct way to subset
    conf_lasso2 <- predict(conf_lasso_model2, newx = v, s = "lambda.min")
    
    all_lasso_model <- cv.glmnet(v, y0)
    all_lasso <- predict(all_lasso_model, newx = v, s = "lambda.min")

    pl_model <- lm(muhat ~ ., data = vdf, subset = (s == 3))
    pl <- predict(pl_model, newdata = vdf)

    bc_model <- lm(bchat ~ ., data = vdf, subset = (s == 3))
    bc <- predict(bc_model, newdata = vdf)

    bct_model <- lm(bc_true ~ ., data = vdf, subset = (s == 3))
    bct <- predict(bct_model, newdata = vdf)
    
    pl_lasso_model <- cv.glmnet(v, muhat, subset = (s == 3))
    pl_lasso <- predict(pl_lasso_model, newx = v, s = "lambda.min")

    bc_lasso_model <- cv.glmnet(v, bchat, subset = (s == 3))
    bc_lasso <- predict(bc_lasso_model, newx = v, s = "lambda.min")

    bct_lasso_model <- cv.glmnet(v, bc_true, subset = (s == 3))
    bct_lasso <- predict(bct_lasso_model, newx = v, s = "lambda.min")


    # gamma
    # alpha
    # dim_z
    # beta
    results <- rbind(results, tibble(
      "mse" = c(
        mean((conf - nu)[s == 4]^2),
        mean((pl - nu)[s == 4]^2),
        mean((bc - nu)[s == 4]^2),
        mean((bct - nu)[s == 4]^2),
        mean((conf_lasso - nu)[s == 4]^2),
        mean((conf_lasso2 - nu)[s == 4]^2),
        mean((pl_lasso - nu)[s == 4]^2),
        mean((bc_lasso - nu)[s == 4]^2),
        mean((bct_lasso - nu)[s == 4]^2),
        mean((all_lasso - nu)[s == 4]^2)
      ),
      "method" = c("conf", "pl", "bc", "bct", "conf_lasso", "conf_lasso2",
                   "pl_lasso", "bc_lasso", "bct_lasso", "all_lasso"),
      "dim" = d,
      "sim" = sim_num
    ))
  }
}

#summary(abs((mu0-nu)[s==4]))
# mean((mu0-nu)[s==4]^2)
#results

# summary((conf-nu)[s==4 & a ==1])
```


```{r}
results %>%
  dplyr::group_by(dim, method) %>%
  dplyr::summarise(mmse = mean(mse),
                   low = mean(mse) - 1.96*sqrt(var(mse)/n()),
                   high = mean(mse) + 1.96*sqrt(var(mse)/n()),
                   ) %>%
  ggplot(aes(x = method, y = mmse, color = method)) +geom_point() + geom_errorbar(aes(ymin = low, ymax = high))  + facet_grid(dim~., scales = "free_y")
```

```{r}
results %>%
  dplyr::group_by(dim, method) %>%
  dplyr::summarise(mmse = mean(mse),
                   low = mean(mse) - 1.96*sqrt(var(mse)/n()),
                   high = mean(mse) + 1.96*sqrt(var(mse)/n()),
                   ) %>%
  filter(method %in% c("all_lasso", "conf_lasso")) %>%
  ggplot(aes(x = method, y = mmse, color = method)) +geom_point() + geom_errorbar(aes(ymin = low, ymax = high))  + facet_grid(dim~., scales = "free_y")
```
